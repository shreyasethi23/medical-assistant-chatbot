{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c862d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "890b97db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/ShreyaSethi/Desktop/Python/Projects/medical-assistant-chatbot/data/images\"\n",
    "\n",
    "# Define image transformations for training and validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),   # Data augmentation for training\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Imagenet mean\n",
    "                             [0.229, 0.224, 0.225])  # Imagenet std\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51341b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# Create DataLoader objects to iterate over datasets in batches\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32,\n",
    "                             shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val']}\n",
    "\n",
    "# Sizes of datasets\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "# Class names (e.g., ['Normal', 'Pneumonia'])\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0ccc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['NORMAL', 'PNEUMONIA']\n",
      "Train dataset size: 5216\n",
      "Validation dataset size: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes:\", class_names)\n",
    "print(\"Train dataset size:\", dataset_sizes['train'])\n",
    "print(\"Validation dataset size:\", dataset_sizes['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86cbc735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ShreyaSethi/Desktop/Python/Projects/medical-assistant-chatbot/venv/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ShreyaSethi/Desktop/Python/Projects/medical-assistant-chatbot/venv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pretrained ResNet18\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace the final fully connected layer to match number of classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Scheduler to decrease learning rate by factor of 0.1 every 7 epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e8162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=15):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluation mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            # Deep copy the best model weights\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "    print(f\"Best val accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b3748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloaders['val']:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce4eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'pneumonia_model.pth')\n",
    "print(\"Model saved as pneumonia_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af33cb51",
   "metadata": {},
   "source": [
    "### INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8263a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "data_dir = \"/Users/ShreyaSethi/Desktop/Python/Projects/medical-assistant-chatbot/data/images\"\n",
    "test_root = os.path.join(data_dir, \"test\")\n",
    "\n",
    "image_paths = []\n",
    "for root, dirs, files in os.walk(test_root):\n",
    "    print(f\"Scanning {root}...\")\n",
    "    for file in files:\n",
    "        if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            full_path = os.path.join(root, file)\n",
    "            print(f\"Found image: {full_path}\")\n",
    "            image_paths.append(full_path)\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# Inference loop with result collection\n",
    "for img_path in image_paths:\n",
    "    true_label = os.path.basename(os.path.dirname(img_path))\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    transform = data_transforms['val']\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_class = class_names[predicted.item()]\n",
    "\n",
    "    results.append({\n",
    "        \"filename\": os.path.basename(img_path),\n",
    "        \"true_label\": true_label,\n",
    "        \"predicted_label\": predicted_class\n",
    "    })\n",
    "\n",
    "# 1. Save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\"Saved predictions to test_predictions.csv\")\n",
    "\n",
    "# 2. Compute Accuracy\n",
    "correct = sum([row[\"true_label\"] == row[\"predicted_label\"] for row in results])\n",
    "total = len(results)\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "\n",
    "# 3. Visualize sample predictions\n",
    "print(\"\\nðŸŽ¨ Visualizing 6 predictions:\")\n",
    "sample = random.sample(results, min(6, total))\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, row in enumerate(sample):\n",
    "    img = Image.open(os.path.join(test_root, row[\"true_label\"], row[\"filename\"])).convert(\"RGB\")\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    color = \"green\" if row[\"true_label\"] == row[\"predicted_label\"] else \"red\"\n",
    "    plt.title(f\"True: {row['true_label']}\\nPred: {row['predicted_label']}\", color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495d925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c06a08-043e-4b53-8455-c0ca0c6f340c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2839e-aa35-4528-b968-2c127c7733d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
